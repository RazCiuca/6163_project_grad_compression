

- also return the distance with the summed gradient to see what that gives
- plot predicted growth of cosine distance for completely random vectors in the given dimention
- plot the growth for a random sequence of actions of given length
- growth of actions sampled by the model?
- random weight directions reconstruct?
- generalize to other architecture?
- plot reward along line search

To obtain:

todo: write the script for the line search along the compressed gradient direction.

todo now: make all the graphs required

- videos of compressed trajs for Ant with quad, net, using the alphas and not using alphas
    -> use the filenames of the compressed videos to make the graphs

- Plot of the cosine similarity as a function of traj_len for Ant with quad and net, both for using alphas and not using alphas
- Plot the average reward for Ant with quad and net, along a line search in the direction pointed to by the compressed gradient
- Plot the cosine similarity for Ant for quad/net with a random weight vector as a function of traj_len
- plot the cos.sim. of all the trajectories found with quad on ant, to net, and see how the cosine similarity varies.





















=================================================
Here's the training loop with policy gradients:
=================================================

- define a model which parametrises our policy
    -> here this is an MLP which outputs a beta distribution for each action dimension
- use that model to collect samples from the environment
    -> i.e. call sample_trajectories from utils, possibly with a vectorized environment to be more efficient
- add those samples to the replay buffer, tagging them with the total probability assigned to them with the current model
    -> and tag them with the total return
    -> the replay buffer needs to eliminate samples with low importance sampling ratios
    -> it needs to keep around the running average of the inputs, to normalize them
        -> actually the network should have those parameters within itself as a layer, to avoid modifying all data in the replaybuffer
- enter a training loop where we sample from the entire replay buffer and train our network using off-policy gradient
    -> sample a batch of trajectories from replay buffer
    -> define the loss = -sum_i G_i log(total prob of traj_i)
    -> differentiate the loss to get gradients

- PolicyGradientAgent.py
    - stores the MLP
    - stores the optimizer
    - stores a normalization routine with stored mean and variance
    - stores the loss computation and sampling from Beta distribution routines

- Trainer.py
    - stores the ReplayBuffer
    - stores the Agent instance
    - does the env_interact -> SGD_loop_through_data -> env_interact macro loop

- ReplayBuffer.py
    -

- train_vanilla_policyGradAgent.py
    - creates the environments we want to train on
    - loops through the agents we want to train and trains them, storing what we want

- TrajCompressor.py

